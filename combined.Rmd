---
title: "biostats_final_combined"
author: "Miao Fu"
date: "2023-12-07"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, message=FALSE,warning=FALSE)
library(tidyverse)
library(dplyr)
library(patchwork)
library(corrplot)
library(gtsummary)
library(tidyr)
library(leaps)
library(glmnet)
library(olsrr)
library(MASS)
library(caret)
```

```{r}
# read datafile
df = read_csv("data/Project_1_data.csv") |> 
  janitor::clean_names() |>
  mutate(
    wkly_study_hours = ifelse(
      wkly_study_hours == "10-May", "5-10", wkly_study_hours)
  )|>
  na.omit()

# Transforming categorical variables to factors
df_transformed <- df |> 
  mutate(
    gender = as.factor(gender),
    ethnic_group = as.factor(ethnic_group),
    parent_educ = factor(parent_educ,
                         levels= c("some high school", "high school", "associate's degree", "some college", "bachelor's degree", "master's degree")),
    lunch_type = as.factor(lunch_type),
    test_prep = as.factor(test_prep),
    parent_marital_status = as.factor(parent_marital_status),
    practice_sport = factor(practice_sport,
                            levels = c("never", "sometimes", "regularly")),
    is_first_child = factor(is_first_child),
    transport_means = as.factor(transport_means),
    wkly_study_hours = factor(wkly_study_hours,
                              levels = c("< 5", "5-10", "> 10"))
  )

# converting categorical variable to numeric variables
df_num=df|>
  mutate(
    gender = as.numeric(factor(gender)),
    ethnic_group = as.numeric(factor(ethnic_group)),
    parent_educ = as.numeric(factor(
      parent_educ,levels= c("some high school", "high school", 
                            "associate's degree", "some college", 
                            "bachelor's degree", "master's degree"))),
    lunch_type = as.numeric(factor(lunch_type)), 
    test_prep = as.numeric(factor(test_prep)),
    parent_marital_status = as.numeric(factor(parent_marital_status)), 
    practice_sport = as.numeric(
      factor(practice_sport, levels = c("never", "sometimes", "regularly"))), 
    is_first_child = as.numeric(factor(is_first_child)),
    transport_means = as.numeric(as.factor(transport_means)),
     wkly_study_hours = as.numeric(factor(wkly_study_hours,
                              levels = c("< 5", "5-10", "> 10")))
  )
```

# Descriptive summary statistics for all variables 

Two table with summary information on the descriptive statistics of all variables are listed below. The frequency and percentage of each categories in each categorical variable is listed out. For each numeric variable, the table includes values of mean, median, standard deviation, minimum, maximum, Q1 and Q3 values. 
```{r, include=FALSE}
# categorical
sum_stats_cat = function(data) {
  results = list()
  
  for (col in colnames(data)) {
    freq_table = table(data[[col]])
    total = sum(freq_table)
    percentages = freq_table / total * 100
    
    results[[col]] = as.data.frame(cbind(freq_table, percentages))
    colnames(results[[col]])=c("count","percent")
  }
  
  return(results)
}

results_cat=df|>
  dplyr::select(-nr_siblings,-math_score,-reading_score,-writing_score)|>
  sum_stats_cat()

for (i in seq_along(results_cat)) {
  print(results_cat[[i]])
}

# numeric
sum_stats_numeric=function(x) {
  results=list()
  
  for (col in colnames(x)){
  table=tibble(
  mean=mean(x[[col]],na.rm=TRUE),
  median=median(x[[col]],na.rm=TRUE),
  sd=sd(x[[col]],na.rm=TRUE),
  minimum=min(x[[col]],na.rm=TRUE),
  maximum=max(x[[col]],na.rm=TRUE),
  q1=quantile(x[[col]], 0.25,na.rm=TRUE),
  q3=quantile(x[[col]], 0.75,na.rm=TRUE))

  results[[col]] = table
  }
  return(results)
}

results_num=df|>
  dplyr::select(nr_siblings,math_score,reading_score,writing_score)|>
  sum_stats_numeric()


```

## Categorical Variables
```{r,echo=FALSE}
do.call(rbind, results_cat)|>
  rownames_to_column(var="variable")|>
  separate(variable,into=c("variable","category"),sep="\\.")|>
  knitr::kable()
```

## Numeric Variables
```{r,echo=FALSE}
do.call(rbind, results_num)|>
  rownames_to_column(var = "variable")|>
  knitr::kable()
```

# Exploration of outcome variables

The outcome of this study includes the following variables: maths scores, reading scores, and writing scores. QQplots of the outcome variables are created to explore the distribution of each score. QQplot compares the quantiles of the data against the quantiles of  a normal distribution. According the plots, majority of the data points of all three scores follow the straight qqline, which indicates they follow the normal distribution. However, there are some deviations from the line on the two ends of the distribution, which indicates the distributions might have heavier tails than normal distribution. To further explore the distribution of outcomes, histograms and boxplots for the scores were incorporated. They are slightly left-tailed but mainly normally distributed. Thus, transformations are tested in the later section to further investigate the variables. 

```{r,echo=FALSE}

# qqplots of 3 scores
qq_math=df|>
  ggplot(aes(sample = math_score)) +
  stat_qq() +
  geom_qq_line()+
  ggtitle("QQ Plot for maths score") +
  xlab("Theoretical Quantiles") +
  ylab("Sample Quantiles")

qq_writing=df|>
  ggplot(aes(sample = writing_score)) +
  stat_qq() +
  geom_qq_line()+
  ggtitle("QQ Plot for writing score") +
  xlab("Theoretical Quantiles") +
  ylab("Sample Quantiles")

qq_reading=df|>
  ggplot(aes(sample = reading_score)) +
  stat_qq() +
  geom_qq_line()+
  ggtitle("QQ Plot for reading score") +
  xlab("Theoretical Quantiles") +
  ylab("Sample Quantiles")

(qq_math+qq_reading)/qq_writing

# distribution for 3 scores

maths_hist=df|>
  dplyr::select(math_score)|>
  ggplot(aes(x=(math_score)))+
  geom_histogram()+
  labs(
    x="Maths Score",
    y="Count"
  )

maths_box=df|>
  dplyr::select(math_score)|>
  ggplot(aes(x=(math_score)))+
  geom_boxplot()+
  labs(
    x="Maths Score"
  )

reading_hist=df|>
  dplyr::select(reading_score)|>
  ggplot(aes(x=(reading_score)))+
  geom_histogram()+
  labs(
    x="Reading Score",
    y="Count"
  )

reading_box=df|>
  dplyr::select(reading_score)|>
  ggplot(aes(x=(reading_score)))+
  geom_boxplot()+
  labs(
    x="Reading Score"
  )

writing_hist=df|>
  dplyr::select(writing_score)|>
  ggplot(aes(x=(writing_score)))+
  geom_histogram()+
  labs(
    x="Writing Score",
    y="Count"
  )

writing_box=df|>
  dplyr::select(writing_score)|>
  ggplot(aes(x=(writing_score)))+
  geom_boxplot()+
  labs(
    x="Writing Score"
  )

(maths_hist+maths_box)/(reading_hist+reading_box)/(writing_hist+writing_box)
```

# Exploration of predictor variables

```{r}
par(mfrow=c(2,3))
barplot(table(df_transformed$gender), main='Gender')
barplot(table(df_transformed$ethnic_group), main='Ethnic Group')
barplot(table(df_transformed$lunch_type), main='Lunch Type')
barplot(table(df_transformed$test_prep), main='Test Prep')
barplot(table(df_transformed$parent_educ), main='Parent Education')
barplot(table(df_transformed$parent_marital_status), main='Parent Marital Status')
barplot(table(df_transformed$practice_sport), main='Practice Sports')
barplot(table(df_transformed$is_first_child), main='First Child')
barplot(table(df_transformed$nr_siblings), main='Siblings')
barplot(table(df_transformed$transport_means), main='Transport Means')
barplot(table(df_transformed$wkly_study_hours), main='Weekly Study Hours')

```
From the distribution bar graphs, it was decided that no transformation is needed for any predictor variables. Admittedly, the distributions might not be good references for transformation because the variables are all categorical. 

# Potential transformations for outcome variables 

```{r}
# Log, Sqrt, and Inverse transformation of outcomes
df_eda=df|>
  dplyr::select(math_score,writing_score,reading_score)|>
  mutate(
    lgMath=log(math_score),
    sqMath=sqrt(math_score),
    inMath=1/(math_score),
    lgRead=log(reading_score),
    sqRead=sqrt(reading_score),
    inRead=1/(reading_score),
    lgWrite=log(writing_score),
    sqWrite=sqrt(writing_score),
    inWrite=1/(writing_score),
  )
par(mfrow=c(2,3))
hist(df_eda$lgMath, main="Log(Maths Score)",xlab="Score")
hist(df_eda$sqMath, main="sq(Maths Score)",xlab="Score")
hist(df_eda$inMath, main="in(Maths Score)",xlab="Score")
hist(df_eda$lgRead, main="Log(Reading Score)",xlab="Score")
hist(df_eda$sqRead, main="sq(Reading Score)",xlab="Score")
hist(df_eda$inRead, main="in(Reading Score)",xlab="Score")
hist(df_eda$lgWrite, main="Log(Writing Score)",xlab="Score")
hist(df_eda$sqWrite, main="sq(Writing Score)",xlab="Score")
hist(df_eda$inWrite, main="in(Writing Score)",xlab="Score")

```
Since the distribution of the three scores showed slightly left-tailed, three types of transformations were tested: 1) Natural logarithm 2) Square Root 3) Inverse. The resulting plots are plotted in histograms shown above. There is no apparent improvement on the distribution of the outcome through the three transformations. Thus, the orignial outcome data were chosen to be used in following statistical modeling steps. 

# Pairwise relationships

```{r}
df_num|>
  cor()|>
  corrplot(type = "upper", diag = FALSE)
```

By plotting our the pairwise correlation between variables, there is apparent linearity among the three scores. Other correlation coefficients are relatively small, indicating weak linear relationship between the variables. 

# MLR lm()

```{r MLR models}
# Build the MLR model for Math scores
model_math <- lm(math_score ~ gender + ethnic_group + parent_educ + lunch_type + test_prep + parent_marital_status + practice_sport + is_first_child + nr_siblings + transport_means + wkly_study_hours, data = df_transformed)
model_read <- lm(reading_score ~ gender + ethnic_group + parent_educ + lunch_type + test_prep + parent_marital_status + practice_sport + is_first_child + nr_siblings + transport_means + wkly_study_hours, data = df_transformed)
model_write <- lm(writing_score ~ gender + ethnic_group + parent_educ + lunch_type + test_prep + parent_marital_status + practice_sport + is_first_child + nr_siblings + transport_means + wkly_study_hours, data = df_transformed)
```


## MLR - Math
```{r math}
summary(model_math)
```

### Coefficients and Significance Levels:
* Intercept (44.1006): The expected value of math_score when all other predictors are at their reference level or zero.
* gendermale (5.0855, p < 0.001): Being male is associated with an average increase of 5.0855 points in math_score compared to females, holding all else constant. This is statistically significant.
* ethnic_group: Only ethnic_groupgroup E (11.1752, p < 0.001) is significant, suggesting students in this group score higher in math compared to the reference group.
* parent_educ: The associate's degree (4.9058, p = 0.00584), bachelor's degree (6.6652, p = 0.00140), and master's degree (6.8096, p = 0.00760) are significant and associated with higher math scores compared to the reference category.
* lunch_typestandard (12.3539, p < 0.001): Students with standard lunch type score significantly higher.
* test_prepnone (-4.7717, p < 0.001): Not participating in test preparation is associated with lower math scores.
* parent_marital_status: Married (5.4805, p = 0.00075) and Widowed (7.7944, p = 0.04134) are associated with higher scores.
* practice_sport: Not significant.
* is_first_childyes: Not significant.
* nr_siblings (0.7403, p = 0.05461): A borderline significant positive association with math scores.
* transport_meansschool_bus: Not significant.
* wkly_study_hours: Studying 5-10 hours (3.5394, p = 0.00863) shows a significant positive effect.

### Residuals:
The spread of residuals suggests the errors are somewhat symmetrically distributed around the predicted values, which is a good sign for linear regression assumptions.

### Model Fit:
Residual Standard Error (13.52): Indicates the average difference between the observed values and the values predicted by the model.

### R-squared:
* Multiple R-squared (0.3221): About 32.21% of the variability in math_score is explained by the model.
* Adjusted R-squared (0.2956): Adjusts the R-squared for the number of predictors, a better measure for models with multiple predictors.

### Statistic & p-value
F-statistic (12.18) and p-value (< 2.2e-16): The model is statistically significant, meaning it performs better than a model with no predictors.

## MLR - reading
```{r}
summary(model_read)
```
### Coefficients and Significance Levels:
* **Intercept (60.8028)**: The expected value of `reading_score` when all other predictors are at their reference level or zero.
* **gendermale (-7.6725, p < 0.001)**: Being male is associated with an average decrease of 7.6725 points in `reading_score` compared to females, holding all else constant. This is statistically significant.
* **ethnic_group**: Only `ethnic_groupgroup E` (5.9165, p = 0.013402) is significant, suggesting students in this group score higher in reading compared to the reference group.
* **parent_educ**: The `associate's degree` (4.7948, p = 0.005776), `bachelor's degree` (7.3496, p = 0.000313), and `master's degree` (8.7149, p = 0.000479) are significant and associated with higher reading scores compared to the reference category.
* **lunch_typestandard (8.4374, p < 0.001)**: Students with standard lunch type score significantly higher.
* **test_prepnone (-6.2822, p < 0.001)**: Not participating in test preparation is associated with lower reading scores.
* **parent_marital_statusmarried (5.2439, p = 0.000950)**: Children of married parents score higher.
* **practice_sport**: Not significant.
* **is_first_childyes**: Not significant.
* **nr_siblings (0.3882, p = 0.301309)**: No significant association with reading scores.
* **transport_meansschool_bus**: Not significant.
* **wkly_study_hours**: Studying 5-10 hours (2.6835, p = 0.041104) shows a significant positive effect.

### Residuals:
The spread of residuals suggests the errors are somewhat symmetrically distributed around the predicted values, which is a good sign for linear regression assumptions.

### Model Fit:
* **Residual Standard Error (13.2)**: Indicates the average difference between the observed values and the values predicted by the model.

### R-squared:
* **Multiple R-squared (0.2709)**: About 27.09% of the variability in `reading_score` is explained by the model.
* **Adjusted R-squared (0.2425)**: Adjusts the R-squared for the number of predictors, a better measure for models with multiple predictors.

### Statistic & p-value
* **F-statistic (9.527)** and **p-value (< 2.2e-16)**: The model is statistically significant, meaning it performs better than a model with no predictors.

## MLR - writing
```{r}
summary(model_write) 
```
### Coefficients and Significance Levels:
* **Intercept (57.808758)**: The expected value of `writing_score` when all other predictors are at their reference level or zero.
* **gendermale (-9.268845, p < 0.001)**: Being male is associated with an average decrease of 9.268845 points in `writing_score` compared to females, holding all else constant. This is statistically significant.
* **ethnic_group**: `ethnic_groupgroup D` (5.010576, p = 0.016531) and `ethnic_groupgroup E` (6.018419, p = 0.008673) are significant, suggesting students in these groups score higher in writing compared to the reference group.
* **parent_educ**: `associate's degree` (6.130783, p = 0.000239), `some college` (4.338798, p = 0.010898), `bachelor's degree` (9.217680, p = 2.62e-06), and `master's degree` (11.712279, p = 1.10e-06) are significant and associated with higher writing scores compared to the reference category.
* **lunch_typestandard (9.390698, p < 0.001)**: Students with standard lunch type score significantly higher.
* **test_prepnone (-8.754351, p < 0.001)**: Not participating in test preparation is associated with lower writing scores.
* **parent_marital_statusmarried (5.246610, p = 0.000561)**: Children of married parents score higher.
* **practice_sport**: Not significant.
* **is_first_childyes**: Not significant.
* **nr_siblings (0.546033, p = 0.129340)**: No significant association with writing scores.
* **transport_meansschool_bus**: Not significant.
* **wkly_study_hours**: Studying 5-10 hours (2.802323, p = 0.026048) shows a significant positive effect.

### Residuals:
The spread of residuals suggests the errors are somewhat symmetrically distributed around the predicted values, which is a good sign for linear regression assumptions.

### Model Fit:
* **Residual Standard Error (12.65)**: Indicates the average difference between the observed values and the values predicted by the model.

### R-squared:
* **Multiple R-squared (0.3634)**: About 36.34% of the variability in `writing_score` is explained by the model.
* **Adjusted R-squared (0.3385)**: Adjusts the R-squared for the number of predictors, a better measure for models with multiple predictors.

### Statistic & p-value
* **F-statistic (14.63)** and **p-value (< 2.2e-16)**: The model is statistically significant, meaning it performs better than a model with no predictors.


## Cleaned datasets - updated by Nisha
```{r}
set.seed(555)

step_df = read_csv("data/Project_1_data.csv") |>
  drop_na() |> janitor::clean_names() |>
  mutate(
    wkly_study_hours = ifelse(
      wkly_study_hours == "10-May", "5-10", wkly_study_hours)
  )|>
  mutate(
    gender = as.integer(factor(gender)),
    ethnic_group = as.integer(factor(ethnic_group)),
    parent_educ = as.integer(factor(
      parent_educ,levels= c("some high school", "high school", 
                            "associate's degree", "some college", 
                            "bachelor's degree", "master's degree"))),
    lunch_type = as.integer((factor(lunch_type))), 
    test_prep = as.integer((factor(test_prep))),
    parent_marital_status = as.integer((factor(parent_marital_status))), 
    practice_sport = as.integer((factor(practice_sport, levels = c("never", "sometimes", "regularly")))), 
    is_first_child = as.integer((factor(is_first_child))),
    transport_means = as.integer((factor(transport_means))),
     wkly_study_hours = as.integer((factor(wkly_study_hours,
                              levels = c("< 5", "5-10", "> 10"))))
  )

math_df = dplyr::select(step_df, -c(reading_score, writing_score)) 

reading_df = dplyr::select(step_df, -c(math_score, writing_score))

writing_df = dplyr::select(step_df, -c(reading_score, math_score))
```

## Step-wise: Backwards Elimination

Math Score
```{r}
mult.fit = lm(math_score ~ ., data = math_df)
summary(mult.fit)

# No Transport Means
step1 = update(mult.fit, . ~ . -transport_means)
summary(step1)

# No Is First Child
step2 = update(step1, . ~ . -is_first_child)
summary(step2)

# No Practice Sport
step3 = update(step2, . ~ . -practice_sport)
summary(step3)

# No Parent Marital Status
step4 = update(step3, . ~ . -parent_marital_status)
summary(step4)

# No Number of Siblings
step5 = update(step4, . ~ . -nr_siblings)
summary(step5)

# just use one function
step(mult.fit, direction='backward')
```
With manual elimination, the model we obtained was Math Score ~ Gender + Ethnic
Group + Parent Education + Lunch Type + Test Prep + Weekly Study Hours.

When using the single-function method, the model obtained with the lowest AIC
was Math Score ~ Gender + Ethnic Group + Parent Education + Lunch Type + 
Test Prep + Number of Siblings + Weekly Study Hours.


Reading Score
```{r}
mult.fit = lm(reading_score ~ ., data = reading_df)
summary(mult.fit)

# No Parent Marital Status
step1 = update(mult.fit, . ~ . -parent_marital_status)
summary(step1)

# No Is First Child
step2 = update(step1, . ~ . -is_first_child)
summary(step2)

# No Transport Means
step3 = update(step2, . ~ . -transport_means)
summary(step3)

# No Number of Siblings
step4 = update(step3, . ~ . -nr_siblings)
summary(step4)

# No Practice Sport
step5 = update(step4, . ~ . -practice_sport)
summary(step5)

# No Weekly Study Hours
step6 = update(step5, . ~ . -wkly_study_hours)
summary(step6)

# just use one function
step(mult.fit, direction='backward')
```
With manual elimination, the model we obtained was Reading Score ~ Gender + Ethnic
Group + Parent Education + Lunch Type + Test Prep.

When using the single-function method, the model obtained with the lowest AIC
was Reading Score ~ Gender + Ethnic Group + Parent Education + Lunch Type + 
Test Prep.

Writing Score
```{r}
mult.fit = lm(writing_score ~ ., data = writing_df)
summary(mult.fit)

# No Is First Child
step1 = update(mult.fit, . ~ . -is_first_child)
summary(step1)

# No Practice Sport
step2 = update(step1, . ~ . -practice_sport)
summary(step2)

# No Transport Means
step3 = update(step2, . ~ . -transport_means)
summary(step3)

# No Parent Marital Status
step4 = update(step3, . ~ . -parent_marital_status)
summary(step4)

# No Number of Siblings
step5 = update(step4, . ~ . -nr_siblings)
summary(step5)

# No Weekly Study Hours
step6 = update(step5, . ~ . -wkly_study_hours)
summary(step6)

# just use one function
step(mult.fit, direction='backward')

```
With manual elimination, the model we obtained was Writing Score ~ Gender + Ethnic
Group + Parent Education + Lunch Type + Test Prep.

When using the single-function method, the model obtained with the lowest AIC
was Writing Score ~ Gender + Ethnic Group + Parent Education + Lunch Type + 
Test Prep + Weekly Study Hours.

## Step-wise: Forward Elimination

Math Score
```{r}

mult.fit = lm(math_score ~ ., data = math_df)

### Step 1:  Fit simple linear regressions for all variables,look for the variable with lowest p-value
fit1 = lm(math_score ~ gender, data = step_df)
summary(fit1)
fit2 = lm(math_score ~ ethnic_group, data = step_df)
summary(fit2)
fit3 = lm(math_score ~ parent_educ, data = step_df)
summary(fit3)
fit4 = lm(math_score ~ lunch_type, data = step_df)
summary(fit4)
fit5 = lm(math_score ~ parent_marital_status, data = step_df)
summary(fit5)
fit6 = lm(math_score ~ practice_sport, data = step_df)
summary(fit6)
fit7 = lm(math_score ~ is_first_child, data = step_df)
summary(fit7)
fit8 = lm(math_score ~ nr_siblings, data = step_df)
summary(fit8)
fit9 = lm(math_score ~ transport_means, data = step_df)
summary(fit9)
fit10 = lm(math_score ~ wkly_study_hours, data = step_df)
summary(fit10)
fit11 = lm(math_score ~ test_prep, data = step_df)
summary(fit11)

# Enter first the one with the lowest p-value: Lunch Type
forward1 = lm(math_score ~ lunch_type, data = step_df)
first = summary(forward1)|> broom::tidy()


### Step 2: Enter the one with the lowest p-value in the rest 
fit1 = update(forward1, . ~ . +gender)
summary(fit1)

fit2 = update(forward1, . ~ . +ethnic_group)
summary(fit2)

fit3 = update(forward1, . ~ . +parent_educ)
summary(fit3)

fit4 = update(forward1, . ~ . +parent_marital_status)
summary(fit4)

fit5 = update(forward1, . ~ . +practice_sport)
summary(fit5)

fit6 = update(forward1, . ~ . +is_first_child)
summary(fit6)

fit7 = update(forward1, . ~ . +nr_siblings)
summary(fit7)

fit8 = update(forward1, . ~ . +transport_means)
summary(fit8)

fit9 = update(forward1, . ~ . +wkly_study_hours)
summary(fit9)

fit10 = update(forward1, . ~ . +test_prep)
summary(fit10)


# Enter the one with the lowest p-value: Ethnic Group
forward2 = update(forward1, . ~ . +ethnic_group)
summary(fit2)

### Step 3: Enter the one with the lowest p-value in the rest 
fit1 = update(forward2, . ~ . +gender)
summary(fit1)

fit2 = update(forward2, . ~ . +parent_educ)
summary(fit2)

fit3 = update(forward2, . ~ . +parent_marital_status)
summary(fit3)

fit4 = update(forward2, . ~ . +practice_sport)
summary(fit4)

fit5 = update(forward2, . ~ . +is_first_child)
summary(fit5)

fit6 = update(forward2, . ~ . +nr_siblings)
summary(fit6)

fit7 = update(forward2, . ~ . +transport_means)
summary(fit7)

fit8 = update(forward2, . ~ . +wkly_study_hours)
summary(fit8)

fit9 = update(forward2, . ~ . +test_prep)
summary(fit9)


# Enter the one with the lowest p-value: Test Prep
forward3 = update(forward2, . ~ . + test_prep)
summary(forward3)

### Step 4: Enter the one with the lowest p-value in the rest 
fit1 = update(forward3, . ~ . +gender)
summary(fit1)

fit2 = update(forward3, . ~ . +parent_educ)
summary(fit2)

fit3 = update(forward3, . ~ . +parent_marital_status)
summary(fit3)

fit4 = update(forward3, . ~ . +practice_sport)
summary(fit4)

fit5 = update(forward3, . ~ . +is_first_child)
summary(fit5)

fit6 = update(forward3, . ~ . +nr_siblings)
summary(fit6)

fit7 = update(forward3, . ~ . +transport_means)
summary(fit7)

fit8 = update(forward3, . ~ . +wkly_study_hours)
summary(fit8)


# Enter the one with the lowest p-value: Gender
forward4 = update(forward3, . ~ . + gender)
summary(forward4)

### Step 5: Enter the one with the lowest p-value in the rest 
fit1 = update(forward4, . ~ . +parent_educ)
summary(fit1)

fit2 = update(forward4, . ~ . +parent_marital_status)
summary(fit2)

fit3 = update(forward4, . ~ . +practice_sport)
summary(fit3)

fit4 = update(forward4, . ~ . +is_first_child)
summary(fit4)

fit5 = update(forward4, . ~ . +nr_siblings)
summary(fit5)

fit6 = update(forward4, . ~ . +transport_means)
summary(fit6)

fit7 = update(forward4, . ~ . +wkly_study_hours)
summary(fit7)

# Enter the one with the lowest p-value: Parent Education
forward5 = update(forward4, . ~ . + parent_educ)
summary(forward5)

### Step 6: Enter the one with the lowest p-value in the rest 
fit1 = update(forward5, . ~ . +parent_marital_status)
summary(fit1)

fit2 = update(forward5, . ~ . +practice_sport)
summary(fit2)

fit3 = update(forward5, . ~ . +is_first_child)
summary(fit3)

fit4 = update(forward5, . ~ . +nr_siblings)
summary(fit4)

fit5 = update(forward5, . ~ . +transport_means)
summary(fit5)

fit6 = update(forward5, . ~ . +wkly_study_hours)
summary(fit6)


# Enter the one with the lowest p-value: Weekly Study Hours
forward6 = update(forward5, . ~ . + wkly_study_hours)
summary(forward6)

### Step 7: Enter the one with the lowest p-value in the rest 
fit1 = update(forward6, . ~ . +parent_marital_status)
summary(fit1)

fit2 = update(forward6, . ~ . +practice_sport)
summary(fit2)

fit3 = update(forward6, . ~ . +is_first_child)
summary(fit3)

fit4 = update(forward6, . ~ . +nr_siblings)
summary(fit4)

fit5 = update(forward6, . ~ . +transport_means)
summary(fit5)

# P-value of all new added variables are larger than 0.05, which means that they 
# are not significant predictor, and we stop here.

mult.fit.final = lm(math_score ~ lunch_type + ethnic_group + test_prep + 
    gender + parent_educ + wkly_study_hours, data = step_df)
summary(mult.fit.final)

# fit using one function
intercept_only <- lm (math_score ~ 1, data = math_df)
step(intercept_only, direction = "forward", scope = formula(mult.fit))
```
The model we obtained is Math Score ~ Lunch Type + Ethnic Group + Test Prep +
Gender + Parent Education + Weekly Study Hours.

When using the single-function method, the model obtained with the lowest AIC
was Math Score ~ Lunch Type + Ethnic Group + Test Prep + Gender + Parent
Education + Weekly Study Hours + Number of Siblings.


Reading Score
```{r}
mult.fit = lm(reading_score ~ ., data = reading_df)

### Step 1:  Fit simple linear regressions for all variables,look for the variable with lowest p-value
fit1 = lm(reading_score ~ gender, data = step_df)
summary(fit1)
fit2 = lm(reading_score ~ ethnic_group, data = step_df)
summary(fit2)
fit3 = lm(reading_score ~ parent_educ, data = step_df)
summary(fit3)
fit4 = lm(reading_score ~ lunch_type, data = step_df)
summary(fit4)
fit5 = lm(reading_score ~ parent_marital_status, data = step_df)
summary(fit5)
fit6 = lm(reading_score ~ practice_sport, data = step_df)
summary(fit6)
fit7 = lm(reading_score ~ is_first_child, data = step_df)
summary(fit7)
fit8 = lm(reading_score ~ nr_siblings, data = step_df)
summary(fit8)
fit9 = lm(reading_score ~ transport_means, data = step_df)
summary(fit9)
fit10 = lm(reading_score ~ wkly_study_hours, data = step_df)
summary(fit10)
fit11 = lm(reading_score ~ test_prep, data = step_df)
summary(fit11)

# Enter first the one with the lowest p-value: Lunch Type
forward1 = lm(reading_score ~ lunch_type, data = step_df)
summary(forward1)


### Step 2: Enter the one with the lowest p-value in the rest 
fit1 = update(forward1, . ~ . +gender)
summary(fit1)

fit2 = update(forward1, . ~ . +ethnic_group)
summary(fit2)

fit3 = update(forward1, . ~ . +parent_educ)
summary(fit3)

fit4 = update(forward1, . ~ . +parent_marital_status)
summary(fit4)

fit5 = update(forward1, . ~ . +practice_sport)
summary(fit5)

fit6 = update(forward1, . ~ . +is_first_child)
summary(fit6)

fit7 = update(forward1, . ~ . +nr_siblings)
summary(fit7)

fit8 = update(forward1, . ~ . +transport_means)
summary(fit8)

fit9 = update(forward1, . ~ . +wkly_study_hours)
summary(fit9)

fit10 = update(forward1, . ~ . +test_prep)
summary(fit10)


# Enter the one with the lowest p-value: Gender
forward2 = update(forward1, . ~ . +gender)
summary(fit2)

### Step 3: Enter the one with the lowest p-value in the rest 
fit1 = update(forward2, . ~ . +ethnic_group)
summary(fit1)

fit2 = update(forward2, . ~ . +parent_educ)
summary(fit2)

fit3 = update(forward2, . ~ . +parent_marital_status)
summary(fit3)

fit4 = update(forward2, . ~ . +practice_sport)
summary(fit4)

fit5 = update(forward2, . ~ . +is_first_child)
summary(fit5)

fit6 = update(forward2, . ~ . +nr_siblings)
summary(fit6)

fit7 = update(forward2, . ~ . +transport_means)
summary(fit7)

fit8 = update(forward2, . ~ . +wkly_study_hours)
summary(fit8)

fit9 = update(forward2, . ~ . +test_prep)
summary(fit9)


# Enter the one with the lowest p-value: Test Prep
forward3 = update(forward2, . ~ . + test_prep)
summary(forward3)

### Step 4: Enter the one with the lowest p-value in the rest 
fit1 = update(forward3, . ~ . +ethnic_group)
summary(fit1)

fit2 = update(forward3, . ~ . +parent_educ)
summary(fit2)

fit3 = update(forward3, . ~ . +parent_marital_status)
summary(fit3)

fit4 = update(forward3, . ~ . +practice_sport)
summary(fit4)

fit5 = update(forward3, . ~ . +is_first_child)
summary(fit5)

fit6 = update(forward3, . ~ . +nr_siblings)
summary(fit6)

fit7 = update(forward3, . ~ . +transport_means)
summary(fit7)

fit8 = update(forward3, . ~ . +wkly_study_hours)
summary(fit8)


# Enter the one with the lowest p-value: Parent Education
forward4 = update(forward3, . ~ . + parent_educ)
summary(forward4)

### Step 5: Enter the one with the lowest p-value in the rest 
fit1 = update(forward4, . ~ . +ethnic_group)
summary(fit1)

fit2 = update(forward4, . ~ . +parent_marital_status)
summary(fit2)

fit3 = update(forward4, . ~ . +practice_sport)
summary(fit3)

fit4 = update(forward4, . ~ . +is_first_child)
summary(fit4)

fit5 = update(forward4, . ~ . +nr_siblings)
summary(fit5)

fit6 = update(forward4, . ~ . +transport_means)
summary(fit6)

fit7 = update(forward4, . ~ . +wkly_study_hours)
summary(fit7)

# Enter the one with the lowest p-value: Ethnic Group
forward5 = update(forward4, . ~ . + ethnic_group)
summary(forward5)

### Step 6: Enter the one with the lowest p-value in the rest 
fit1 = update(forward5, . ~ . +parent_marital_status)
summary(fit1)

fit2 = update(forward5, . ~ . +practice_sport)
summary(fit2)

fit3 = update(forward5, . ~ . +is_first_child)
summary(fit3)

fit4 = update(forward5, . ~ . +nr_siblings)
summary(fit4)

fit5 = update(forward5, . ~ . +transport_means)
summary(fit5)

fit6 = update(forward5, . ~ . +wkly_study_hours)
summary(fit6)

# P-value of all new added variables are larger than 0.05, which means that they 
# are not significant predictor, and we stop here.

mult.fit.final = lm(reading_score ~ lunch_type + gender + test_prep + 
                      parent_educ + ethnic_group, data = step_df)
summary(mult.fit.final)

# fit using one function
intercept_only <- lm (reading_score ~ 1, data = reading_df)
step(intercept_only, direction = "forward", scope = formula(mult.fit))
```
The model we obtained is Reading Score ~ Lunch Type + Ethnic Group + Test Prep +
Gender + Parent Education + Weekly Study Hours.

When using the single-function method, the model obtained with the lowest AIC
was Reading Score ~ Lunch Type + Gender + Test Prep + Parent Education + 
Ethnic Group.

Writing Score
```{r}
mult.fit = lm(writing_score ~ ., data = writing_df)

### Step 1:  Fit simple linear regressions for all variables,look for the variable with lowest p-value
fit1 = lm(writing_score ~ gender, data = step_df)
summary(fit1)
fit2 = lm(writing_score ~ ethnic_group, data = step_df)
summary(fit2)
fit3 = lm(writing_score ~ parent_educ, data = step_df)
summary(fit3)
fit4 = lm(writing_score ~ lunch_type, data = step_df)
summary(fit4)
fit5 = lm(writing_score ~ parent_marital_status, data = step_df)
summary(fit5)
fit6 = lm(writing_score ~ practice_sport, data = step_df)
summary(fit6)
fit7 = lm(writing_score ~ is_first_child, data = step_df)
summary(fit7)
fit8 = lm(writing_score ~ nr_siblings, data = step_df)
summary(fit8)
fit9 = lm(writing_score ~ transport_means, data = step_df)
summary(fit9)
fit10 = lm(writing_score ~ wkly_study_hours, data = step_df)
summary(fit10)
fit11 = lm(writing_score ~ test_prep, data = step_df)
summary(fit11)

# Enter first the one with the lowest p-value: Gender
forward1 = lm(writing_score ~ gender, data = step_df)
summary(forward1)


### Step 2: Enter the one with the lowest p-value in the rest 
fit1 = update(forward1, . ~ . +ethnic_group)
summary(fit1)

fit2 = update(forward1, . ~ . +parent_educ)
summary(fit2)

fit3 = update(forward1, . ~ . +parent_marital_status)
summary(fit3)

fit4 = update(forward1, . ~ . +practice_sport)
summary(fit4)

fit5 = update(forward1, . ~ . +is_first_child)
summary(fit5)

fit6 = update(forward1, . ~ . +nr_siblings)
summary(fit6)

fit7 = update(forward1, . ~ . +transport_means)
summary(fit7)

fit8 = update(forward1, . ~ . +wkly_study_hours)
summary(fit8)

fit9 = update(forward1, . ~ . +test_prep)
summary(fit9)

fit10 = update(forward1, . ~ . +lunch_type)
summary(fit10)


# Enter the one with the lowest p-value: Lunch Type
forward2 = update(forward1, . ~ . +lunch_type)
summary(fit2)

### Step 3: Enter the one with the lowest p-value in the rest 
fit1 = update(forward1, . ~ . +ethnic_group)
summary(fit1)

fit2 = update(forward1, . ~ . +parent_educ)
summary(fit2)

fit3 = update(forward1, . ~ . +parent_marital_status)
summary(fit3)

fit4 = update(forward1, . ~ . +practice_sport)
summary(fit4)

fit5 = update(forward1, . ~ . +is_first_child)
summary(fit5)

fit6 = update(forward1, . ~ . +nr_siblings)
summary(fit6)

fit7 = update(forward1, . ~ . +transport_means)
summary(fit7)

fit8 = update(forward1, . ~ . +wkly_study_hours)
summary(fit8)

fit9 = update(forward1, . ~ . +test_prep)
summary(fit9)

# Enter the one with the lowest p-value: Test Prep
forward3 = update(forward2, . ~ . + test_prep)
summary(forward3)

### Step 4: Enter the one with the lowest p-value in the rest 
fit1 = update(forward3, . ~ . +ethnic_group)
summary(fit1)

fit2 = update(forward3, . ~ . +parent_educ)
summary(fit2)

fit3 = update(forward3, . ~ . +parent_marital_status)
summary(fit3)

fit4 = update(forward3, . ~ . +practice_sport)
summary(fit4)

fit5 = update(forward3, . ~ . +is_first_child)
summary(fit5)

fit6 = update(forward3, . ~ . +nr_siblings)
summary(fit6)

fit7 = update(forward3, . ~ . +transport_means)
summary(fit7)

fit8 = update(forward3, . ~ . +wkly_study_hours)
summary(fit8)


# Enter the one with the lowest p-value: Parent Education
forward4 = update(forward3, . ~ . + parent_educ)
summary(forward4)

### Step 5: Enter the one with the lowest p-value in the rest 
fit1 = update(forward4, . ~ . +ethnic_group)
summary(fit1)

fit2 = update(forward4, . ~ . +parent_marital_status)
summary(fit2)

fit3 = update(forward4, . ~ . +practice_sport)
summary(fit3)

fit4 = update(forward4, . ~ . +is_first_child)
summary(fit4)

fit5 = update(forward4, . ~ . +nr_siblings)
summary(fit5)

fit6 = update(forward4, . ~ . +transport_means)
summary(fit6)

fit7 = update(forward4, . ~ . +wkly_study_hours)
summary(fit7)

# Enter the one with the lowest p-value: Ethnic Group
forward5 = update(forward4, . ~ . + ethnic_group)
summary(forward5)

### Step 6: Enter the one with the lowest p-value in the rest 
fit1 = update(forward5, . ~ . +parent_marital_status)
summary(fit1)

fit2 = update(forward5, . ~ . +practice_sport)
summary(fit2)

fit3 = update(forward5, . ~ . +is_first_child)
summary(fit3)

fit4 = update(forward5, . ~ . +nr_siblings)
summary(fit4)

fit5 = update(forward5, . ~ . +transport_means)
summary(fit5)

fit6 = update(forward5, . ~ . +wkly_study_hours)
summary(fit6)

# P-value of all new added variables are larger than 0.05, which means that they 
# are not significant predictor, and we stop here.


mult.fit.final = lm(writing_score ~ gender + lunch_type + test_prep + 
                      parent_educ + ethnic_group, data = step_df)
summary(mult.fit.final)

# fit using one function
intercept_only <- lm (writing_score ~ 1, data = writing_df)
fitt = step(intercept_only, direction = "forward", scope = formula(mult.fit))
summary(fitt)
```
The model we obtained is Writing Score ~ Lunch Type + Ethnic Group + Test Prep +
Gender + Parent Education + Weekly Study Hours.

When using the single-function method, the model obtained with the lowest AIC
was Writing Score ~ Gender + Lunch Type + Test Prep + Parent Education + Ethnic
Group + Weekly Study Hours.

It seems that when using the single-function method, for all scores and for both
forwards and backwards elimination, extra variables were included despite their 
individual p-values > 0.05; therefore the manually-selected models should be 
used for comparison and validation.


## Criteria-based approach - Adjusted R^2, Cp, BIC
(Note: BIC has a larger penalty, leading to less predictors present within 
the model.)

Math Score
```{r}
# perform best subset selection
best_subset <- regsubsets(math_score ~ ., math_df, nvmax = 11)
results <- summary(best_subset)

# extract and plot results
tibble(predictors = 1:11,
       adj_R2 = results$adjr2,
       Cp = results$cp,
       BIC = results$bic) |>
  gather(statistic, value, -predictors) |>
  ggplot(aes(predictors, value, color = statistic)) +
  geom_line(show.legend = F) +
  geom_point(show.legend = F) +
  facet_wrap(~ statistic, scales = "free")

results$which[7,]|>print()
```
To predict math score, the adjusted R^2 statistic, Cp, and BIC plots in combination show that a 7-variable model is optimal. 
The predictors selected are: gender, ethnic_group, parent_educ, lunch_type, test_prep, nr_siblings, and wkly study_hours. 

Reading Score
```{r}
best_subset <- regsubsets(reading_score ~ ., reading_df, nvmax = 11)
results <- summary(best_subset)

tibble(predictors = 1:11,
       adj_R2 = results$adjr2,
       Cp = results$cp,
       BIC = results$bic) %>%
  gather(statistic, value, -predictors) %>%
  ggplot(aes(predictors, value, color = statistic)) +
  geom_line(show.legend = F) +
  geom_point(show.legend = F) +
  facet_wrap(~ statistic, scales = "free")

results$which[5,]|>print()
```
To predict reading score, the adjusted R^2 statistic and Cp and BIC plots shows that a 5-variable model is optimal. 
The predictors selected are: gender, ethnic_grouop, parent_educ, lunch_type, test_prep, practice_sport, and wkly_study_hours. 

Writing Score
```{r}
best_subset <- regsubsets(writing_score ~ ., writing_df, nvmax = 11)
results <- summary(best_subset)

tibble(predictors = 1:11,
       adj_R2 = results$adjr2,
       Cp = results$cp,
       BIC = results$bic) %>%
  gather(statistic, value, -predictors) %>%
  ggplot(aes(predictors, value, color = statistic)) +
  geom_line(show.legend = F) +
  geom_point(show.legend = F) +
  facet_wrap(~ statistic, scales = "free")

results$which[5,]|>print()
```
To predict writing score, the adjusted R^2, Cp, and BIC statistics show that a 5-variable model is optimal. \
The predictors selected are: gender, ethnic_group, parent_educ, lunch_type, test_prep

Limitation: noting that the plots maximum and minimum are not that obvious.

## LASSO approach - 

Maths score:
```{r}
# Find the best lambda
math_lasso=step_df|>
  dplyr::select(-reading_score,-writing_score)|>
  dplyr::select(math_score,everything())

lambda_seq = 10^seq(-3, 0, by = .1)
set.seed(1)
cv_object = cv.glmnet(as.matrix(math_lasso[2:12]),math_lasso$math_score, lambda = lambda_seq, nfolds = 5)

tibble(lambda = cv_object$lambda,
       mean_cv_error = cv_object$cvm) |>
  ggplot(aes(x = lambda, y = mean_cv_error)) +
  geom_point()

# Use the best lambda to model
math_model_lasso=glmnet(as.matrix(math_lasso[2:12]),math_lasso$math_score,lambda=cv_object$lambda.min)
coef(math_model_lasso)
```

Reading score:

```{r}
read_lasso=step_df|>
  dplyr::select(-math_score,-writing_score)|>
  dplyr::select(reading_score,everything())

lambda_seq = 10^seq(-3, 0, by = .1)
set.seed(2)
cv_object = cv.glmnet(as.matrix(read_lasso[2:12]),read_lasso$reading_score, lambda = lambda_seq, nfolds =5)

tibble(lambda = cv_object$lambda,
       mean_cv_error = cv_object$cvm) |>
  ggplot(aes(x = lambda, y = mean_cv_error)) +
  geom_point()

# Use the best lambda to model
read_model_lasso=glmnet(as.matrix(read_lasso[2:12]),read_lasso$reading_score,lambda=cv_object$lambda.min)
coef(read_model_lasso)
```

Writing score:

```{r}
write_lasso=df_num|>
  dplyr::select(-reading_score,-math_score)|>
  dplyr::select(writing_score,everything())

lambda_seq = 10^seq(-3, 0, by = .1)
set.seed(2)
cv_object = cv.glmnet(as.matrix(write_lasso[2:12]),write_lasso$writing_score, lambda = lambda_seq, nfolds =5)

tibble(lambda = cv_object$lambda,
       mean_cv_error = cv_object$cvm) |>
  ggplot(aes(x = lambda, y = mean_cv_error)) +
  geom_point()

# Use the best lambda to model
write_model_lasso=glmnet(as.matrix(write_lasso[2:12]),write_lasso$writing_score,lambda=cv_object$lambda.min)
coef(write_model_lasso)
```
# Cross Validation


# Leverage one score 

Since according to the corr plot, there are strong collinearity between the scores. We can just add one score (reading_score for example) to another score's model (maths_score for example) and see if the resulting model is improved. 



